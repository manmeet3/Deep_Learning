{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUqMQnflu6gk"
   },
   "source": [
    "## Requirements \n",
    "* (done) Apply **mini-batch gradient descent** with appropriate batch size\n",
    "* (done) Use appropriate **learning rate** (can be adaptive per epoch)\n",
    "* (todo) Apply **dropout** - find appropriate dropout rate at each layer\n",
    "* (done) Initialize random **weights** properly before training\n",
    "* (done) Do basic image **augmentation** of training data using Keras\n",
    "* (todo -- currently only 2 layers after input) Use **3 or more layers** with appropriate **number of neurons** per layer\n",
    "* (done) Use **relu activation layer** in the right places\n",
    "* (done) **Normalize and scale** the input before training with Keras\n",
    "* (todo) Include **metrics**: testing, training accuracy validation curves and confusion matrix\n",
    "* (todo) Display top common errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h46hwvalkXY3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[0:2000]\n",
    "y_train = y_train[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-504-e283d41d8210>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "g = plt.imshow(x_train[3][:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset augmentation\n",
    "\n",
    "Using Keras Datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = x_train[0:500]\n",
    "y_train_augmented = y_train[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = x_train_augmented.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 28, 28, 1)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment 1k training dataset images\n",
    "# Reference: https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use datagen.flow to obtain the entire batch at once and prepend to original dataset.\n",
    "x_train2 = None\n",
    "for x_batch, y_batch in datagen.flow(x_train_augmented, y_train_augmented, batch_size=len(x_train_augmented)):\n",
    "    x_train2 = x_batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-shape the augmented dataset into (num_instances, width, height)\n",
    "x_train_augmented = x_train2.reshape(len(x_train_augmented), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:500] = x_train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = plt.imshow(x_train_augmented[3][:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above two images look the same.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(x_train_augmented[5], x_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like Keras datagen actually did some augmentation. Not enough to be visible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization\n",
    "The training data is 1 3D array of (digit_label, pixel width, pixel height). For our MLP to run gradient descent, the width and height must be converted into a vector of 784 pixels.\n",
    "\n",
    "This is accomplished using numpy's reshape() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# Scale input (examples, width, height) --> (examples, width*height)\n",
    "x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32')\n",
    "x_test = x_test.reshape((x_test.shape[0], num_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode the labels\n",
    "\n",
    "One hot encoding essentially transforms the categorical values into a matrix where their existence or absence is marked by 1 or 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions\n",
    "\n",
    "* Relu\n",
    "* Mini-batch gradient descent based train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TApeoxxEkY-r"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vehw6MM7q-8j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs a 3 layer mini batch gradient descent \n",
    "\n",
    "@ params: images\n",
    "@ retval: void\n",
    "''' \n",
    "def train(images, labels, test_images, test_labels):\n",
    "    batch_size = 100\n",
    "\n",
    "    alpha, iterations = (0.001, 300)\n",
    "    \n",
    "    # MNIST dataset specific settings and hidden layer neuron size\n",
    "    pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "    # Weight initialization for various layers -- # of neurons based on the tuple passed to np.random\n",
    "    weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "    #weights_1_2 = 0.2 * np.random.random((hidden_size, hidden_size//2)) - 0.1\n",
    "    weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        error, correct_cnt = (0.0, 0)\n",
    "      \n",
    "        for i in range(int(len(images) / batch_size)):\n",
    "            batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
    "\n",
    "            layer_0 = images[batch_start:batch_end]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
    "            for k in range(batch_size):\n",
    "                correct_cnt += int(np.argmax(layer_2[k:k+1])) == np.argmax(labels[batch_start+k:batch_start+k+1])\n",
    "\n",
    "                layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
    "                layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "                layer_1_delta *= dropout_mask\n",
    "\n",
    "                weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "                weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "        if(j%10 == 0):\n",
    "            test_error = 0.0\n",
    "            test_correct_cnt = 0\n",
    "\n",
    "            for i in range(len(test_images)):\n",
    "                layer_0 = test_images[i:i+1]\n",
    "                layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "                layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "                test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                         np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "            sys.stdout.write(\"\\n\" + \\\n",
    "            \"I:\" + str(j) + \\\n",
    "            \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "            \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "            \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "            \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.742 Test-Acc:0.546 Train-Err:1.139 Train-Acc:0.1995\n",
      "I:10 Test-Err:0.553 Test-Acc:0.7794 Train-Err:0.709 Train-Acc:0.5325\n",
      "I:20 Test-Err:0.501 Test-Acc:0.7923 Train-Err:0.670 Train-Acc:0.5715\n",
      "I:30 Test-Err:0.467 Test-Acc:0.8054 Train-Err:0.645 Train-Acc:0.5745\n",
      "I:40 Test-Err:0.445 Test-Acc:0.807 Train-Err:0.619 Train-Acc:0.614\n",
      "I:50 Test-Err:0.443 Test-Acc:0.8168 Train-Err:0.627 Train-Acc:0.6085\n",
      "I:60 Test-Err:0.443 Test-Acc:0.8153 Train-Err:0.622 Train-Acc:0.604\n",
      "I:70 Test-Err:0.432 Test-Acc:0.8126 Train-Err:0.617 Train-Acc:0.601\n",
      "I:80 Test-Err:0.436 Test-Acc:0.8113 Train-Err:0.617 Train-Acc:0.605\n",
      "I:90 Test-Err:0.430 Test-Acc:0.8091 Train-Err:0.611 Train-Acc:0.608\n",
      "I:100 Test-Err:0.435 Test-Acc:0.8126 Train-Err:0.611 Train-Acc:0.6055\n",
      "I:110 Test-Err:0.431 Test-Acc:0.8089 Train-Err:0.598 Train-Acc:0.6085\n",
      "I:120 Test-Err:0.425 Test-Acc:0.8158 Train-Err:0.601 Train-Acc:0.6155\n",
      "I:130 Test-Err:0.428 Test-Acc:0.8161 Train-Err:0.607 Train-Acc:0.6115\n",
      "I:140 Test-Err:0.421 Test-Acc:0.8172 Train-Err:0.598 Train-Acc:0.6145\n",
      "I:150 Test-Err:0.418 Test-Acc:0.8172 Train-Err:0.598 Train-Acc:0.6165\n",
      "I:160 Test-Err:0.421 Test-Acc:0.8124 Train-Err:0.595 Train-Acc:0.62\n",
      "I:170 Test-Err:0.419 Test-Acc:0.8168 Train-Err:0.601 Train-Acc:0.6105\n",
      "I:180 Test-Err:0.416 Test-Acc:0.8194 Train-Err:0.598 Train-Acc:0.623\n",
      "I:190 Test-Err:0.413 Test-Acc:0.8186 Train-Err:0.586 Train-Acc:0.629\n",
      "I:200 Test-Err:0.414 Test-Acc:0.8146 Train-Err:0.591 Train-Acc:0.625\n",
      "I:210 Test-Err:0.419 Test-Acc:0.8176 Train-Err:0.593 Train-Acc:0.6185\n",
      "I:220 Test-Err:0.411 Test-Acc:0.8026 Train-Err:0.591 Train-Acc:0.639\n",
      "I:230 Test-Err:0.417 Test-Acc:0.8058 Train-Err:0.587 Train-Acc:0.633\n",
      "I:240 Test-Err:0.412 Test-Acc:0.815 Train-Err:0.590 Train-Acc:0.6285\n",
      "I:250 Test-Err:0.416 Test-Acc:0.8119 Train-Err:0.601 Train-Acc:0.6265\n",
      "I:260 Test-Err:0.415 Test-Acc:0.8168 Train-Err:0.590 Train-Acc:0.635\n",
      "I:270 Test-Err:0.418 Test-Acc:0.8037 Train-Err:0.590 Train-Acc:0.6335\n",
      "I:280 Test-Err:0.411 Test-Acc:0.813 Train-Err:0.588 Train-Acc:0.6385\n",
      "I:290 Test-Err:0.410 Test-Acc:0.8148 Train-Err:0.589 Train-Acc:0.6335"
     ]
    }
   ],
   "source": [
    "train(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "### Training\n",
    "\n",
    "### Testing\n",
    "\n",
    "### Confusion matrix\n",
    "\n",
    "### Most common errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Asg1_graded.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
