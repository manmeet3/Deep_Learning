{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUqMQnflu6gk"
   },
   "source": [
    "## Requirements \n",
    "* (done) Apply **mini-batch gradient descent** with appropriate batch size\n",
    "* (done) Use appropriate **learning rate** (can be adaptive per epoch)\n",
    "* (do per layer) Apply **dropout** - find appropriate dropout rate at each layer\n",
    "* (done - try more) Initialize random **weights** properly before training\n",
    "* Do basic image **augmentation** of training data using Keras\n",
    "* (todo -- currently only 2 layers after input) Use **3 or more layers** with appropriate **number of neurons** per layer\n",
    "* (pre-done) Use **relu activation layer** in the right places (given in example code)\n",
    "* (done) **Normalize and scale** the input before training with Keras\n",
    "* Include **metrics**: testing, training accuracy and a confusion matrix\n",
    "* Display top common errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h46hwvalkXY3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is 1 3D array of (digit_label, pixel width, pixel height). For our MLP to run gradient descent, the width and height must be converted into a vector of 784 pixels.\n",
    "\n",
    "This is accomplished using numpy's reshape() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "# Reshape (examples, width, height) --> (examples, width*height)\n",
    "x_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32')\n",
    "x_test = x_test.reshape((x_test.shape[0], num_pixels))\n",
    "\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: x_train augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding essentially transforms the categorical values into a matrix where their existence or absence is marked by 1 or 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TApeoxxEkY-r"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "  return output >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vehw6MM7q-8j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Runs a 3 layer mini batch gradient descent \n",
    "\n",
    "@ params: images\n",
    "@ retval: void\n",
    "''' \n",
    "def train(images, labels, test_images, test_labels):\n",
    "    batch_size = 100\n",
    "    # Select learning rate and number of iterations\n",
    "    alpha, iterations = (0.001, 250)\n",
    "    \n",
    "    # MNIST dataset specific settings and hidden layer neuron size\n",
    "    pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "    # Weight initialization for various layers -- # of neurons based on the tuple passed to np.random\n",
    "    weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "    #weights_1_2 = 0.2 * np.random.random((hidden_size, hidden_size/2))\n",
    "    weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        error, correct_cnt = (0.0, 0)\n",
    "      \n",
    "        for i in range(int(len(images) / batch_size)):\n",
    "            batch_start, batch_end = ((i * batch_size), ((i+1) * batch_size))\n",
    "\n",
    "            layer_0 = images[batch_start:batch_end]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "            layer_1 *= dropout_mask * 2\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "            error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
    "            for k in range(batch_size):\n",
    "                correct_cnt += int(np.argmax(layer_2[k:k+1])) == np.argmax(labels[batch_start+k:batch_start+k+1])\n",
    "\n",
    "                layer_2_delta = (labels[batch_start:batch_end] - layer_2) / batch_size\n",
    "                layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "                layer_1_delta *= dropout_mask\n",
    "\n",
    "                weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "                weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "        '''sys.stdout.write(\"\\r\" + \\\n",
    "                      \" I:\" + str(j) +\\\n",
    "                      \" Error:\" + str(error/float(len(images)))[0:5] +\\\n",
    "                      \" Correct:\" + str(correct_cnt/float(len(images))))'''\n",
    "        if(j%10 == 0):\n",
    "            test_error = 0.0\n",
    "            test_correct_cnt = 0\n",
    "\n",
    "            for i in range(len(test_images)):\n",
    "                layer_0 = test_images[i:i+1]\n",
    "                layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "                layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "                test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "                test_correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                         np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "            sys.stdout.write(\"\\n\" + \\\n",
    "            \"I:\" + str(j) + \\\n",
    "            \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "            \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "            \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "            \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.809 Test-Acc:0.4125 Train-Err:1.263 Train-Acc:0.148\n",
      "I:10 Test-Err:0.575 Test-Acc:0.7147 Train-Err:0.626 Train-Acc:0.643\n",
      "I:20 Test-Err:0.516 Test-Acc:0.7521 Train-Err:0.547 Train-Acc:0.73\n",
      "I:30 Test-Err:0.482 Test-Acc:0.777 Train-Err:0.511 Train-Acc:0.743\n",
      "I:40 Test-Err:0.464 Test-Acc:0.7924 Train-Err:0.483 Train-Acc:0.753\n",
      "I:50 Test-Err:0.452 Test-Acc:0.7925 Train-Err:0.458 Train-Acc:0.782\n",
      "I:60 Test-Err:0.449 Test-Acc:0.7912 Train-Err:0.467 Train-Acc:0.782\n",
      "I:70 Test-Err:0.445 Test-Acc:0.7974 Train-Err:0.451 Train-Acc:0.799\n",
      "I:80 Test-Err:0.442 Test-Acc:0.7878 Train-Err:0.449 Train-Acc:0.807\n",
      "I:90 Test-Err:0.446 Test-Acc:0.7849 Train-Err:0.436 Train-Acc:0.795\n",
      "I:100 Test-Err:0.442 Test-Acc:0.7883 Train-Err:0.440 Train-Acc:0.793\n",
      "I:110 Test-Err:0.447 Test-Acc:0.789 Train-Err:0.434 Train-Acc:0.811\n",
      "I:120 Test-Err:0.445 Test-Acc:0.7877 Train-Err:0.428 Train-Acc:0.827\n",
      "I:130 Test-Err:0.443 Test-Acc:0.791 Train-Err:0.434 Train-Acc:0.826\n",
      "I:140 Test-Err:0.444 Test-Acc:0.7943 Train-Err:0.435 Train-Acc:0.812\n",
      "I:150 Test-Err:0.438 Test-Acc:0.7965 Train-Err:0.402 Train-Acc:0.835\n",
      "I:160 Test-Err:0.439 Test-Acc:0.8028 Train-Err:0.434 Train-Acc:0.809\n",
      "I:170 Test-Err:0.435 Test-Acc:0.8032 Train-Err:0.420 Train-Acc:0.816\n",
      "I:180 Test-Err:0.438 Test-Acc:0.8092 Train-Err:0.416 Train-Acc:0.83\n",
      "I:190 Test-Err:0.434 Test-Acc:0.8087 Train-Err:0.416 Train-Acc:0.835\n",
      "I:200 Test-Err:0.435 Test-Acc:0.8071 Train-Err:0.410 Train-Acc:0.832\n",
      "I:210 Test-Err:0.434 Test-Acc:0.8087 Train-Err:0.405 Train-Acc:0.837\n",
      "I:220 Test-Err:0.435 Test-Acc:0.803 Train-Err:0.401 Train-Acc:0.836\n",
      "I:230 Test-Err:0.432 Test-Acc:0.8074 Train-Err:0.395 Train-Acc:0.844\n",
      "I:240 Test-Err:0.432 Test-Acc:0.8052 Train-Err:0.404 Train-Acc:0.835\n",
      "I:250 Test-Err:0.433 Test-Acc:0.8085 Train-Err:0.406 Train-Acc:0.845\n",
      "I:260 Test-Err:0.431 Test-Acc:0.8086 Train-Err:0.399 Train-Acc:0.834\n",
      "I:270 Test-Err:0.434 Test-Acc:0.8064 Train-Err:0.390 Train-Acc:0.845\n",
      "I:280 Test-Err:0.437 Test-Acc:0.803 Train-Err:0.391 Train-Acc:0.856\n",
      "I:290 Test-Err:0.433 Test-Acc:0.8014 Train-Err:0.390 Train-Acc:0.839\n",
      "I:300 Test-Err:0.436 Test-Acc:0.7996 Train-Err:0.401 Train-Acc:0.841\n",
      "I:310 Test-Err:0.431 Test-Acc:0.8025 Train-Err:0.390 Train-Acc:0.843\n",
      "I:320 Test-Err:0.430 Test-Acc:0.7999 Train-Err:0.379 Train-Acc:0.851\n",
      "I:330 Test-Err:0.432 Test-Acc:0.8017 Train-Err:0.390 Train-Acc:0.849\n",
      "I:340 Test-Err:0.427 Test-Acc:0.8038 Train-Err:0.392 Train-Acc:0.838\n",
      "I:350 Test-Err:0.427 Test-Acc:0.8031 Train-Err:0.381 Train-Acc:0.856\n",
      "I:360 Test-Err:0.427 Test-Acc:0.8035 Train-Err:0.385 Train-Acc:0.842\n",
      "I:370 Test-Err:0.430 Test-Acc:0.7981 Train-Err:0.386 Train-Acc:0.846\n",
      "I:380 Test-Err:0.425 Test-Acc:0.8076 Train-Err:0.390 Train-Acc:0.844\n",
      "I:390 Test-Err:0.423 Test-Acc:0.803 Train-Err:0.395 Train-Acc:0.847"
     ]
    }
   ],
   "source": [
    "train(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Asg1_graded.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
